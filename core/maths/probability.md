3. Probability
Random Variables: Understanding the behavior and features of random phenomena.
Probability Distributions: Normal, Binomial, Poisson, etc., underpin much of machine learning.
Expectation, Variance, and Covariance: Key statistics for summarizing random variables.
Conditional Probability: Foundation of Bayesian inference.
Joint and Marginal Distributions: Important for understanding the relationships between variables.
Bayes' Theorem: Fundamental for Bayesian methods in machine learning.
Central Limit Theorem: Underpins many statistical methods and machine learning algorithms.


Probability theory: Probability theory is a branch of mathematics that deals with chance events. In machine learning, probability theory is used to model uncertainty, such as in Bayesian inference and probabilistic graphical models.
Statistics: Statistics is a branch of mathematics that deals with data analysis and interpretation. In machine learning, statistics is used to evaluate the performance of models, such as hypothesis testing and confidence intervals.


## Information theory
Information theory is a branch of mathematics that deals with the quantification of information. In machine learning, information theory is used to evaluate the performance of models, such as cross-entropy loss, and to design algorithms, such as data compression and feature selection.
