# Probability, Statistics, and Information Theory for ML

Machine learning (ML) heavily relies on probability, statistics, and information theory to build models that can effectively learn from data and make predictions or decisions. Probability provides a mathematical framework for understanding the chance events and uncertain outcomes, while statistics offers methods for summarizing and describing data. Information theory focuses on the fundamental limits of information processing and transmission.

## Probability
In machine learning, probability theory provides a mathematical framework for understanding and modeling uncertainty. From random variables to Bayes' theorem, probability concepts are essential for making informed predictions and decision-making. We will cover the basics of probability theory and its application in machine learning, laying the groundwork for more advanced topics.

### Topics
- Random Variables: Understanding the behavior and features of random phenomena.
- Probability Distributions: Normal, Binomial, Poisson, etc., underpin much of machine learning.
- Expectation, Variance, and Covariance: Key statistics for summarizing random variables.
- Conditional Independence: A concept that describes the independence of between random variables.
- Conditional Probability: Foundation of Bayesian inference.
- Joint and Marginal Distributions: Important for understanding the relationships between variables.
- Bayes' Theorem: Fundamental for Bayesian methods in machine learning.
- Central Limit Theorem: Underpins many statistical methods and machine learning algorithms.
- Common Probability Distributions: A survey of distributions commonly used in machine learning.

## Statisics

In machine learning, statistical techniques are used to understand patterns, trends, and relationships within datasets, in addition to evaluating machine learning models.

### Topics
- Descriptive Statistics: This area of statistics deals with summarizing and describing the main features of a dataset, such as mean, median, mode, standard deviation, range, etc. Descriptive statistics are useful for getting a quick overview of a dataset and identifying patterns or trends.
- Inferential Statistics: This area of statistics deals with making inferences about a population based on a sample of data. Inferential statistics includes hypothesis testing and confidence intervals, which are used to determine whether a pattern or relationship observed in a sample is likely to hold true for the larger population.
- Bayesian Statistics: This area of statistics deals with probabilistic modeling using Bayes' theorem. Bayesian statistics are used in machine learning for tasks such as parameter estimation, model selection, and prediction.
- Regression Analysis: This area of statistics deals with modeling the relationship between a dependent variable and one or more independent variables. Regression analysis is widely used in machine learning for tasks such as predicting continuous targets, feature selection, and dimensionality reduction.
- Time Series Analysis: This area of statistics deals with analyzing data that is ordered in time. Time series analysis is used in machine learning for tasks such as forecasting, anomaly detection, and segmentation.
- Hypothesis Testing: This area of statistics deals with testing hypotheses about a population parameter based on a sample of data. Hypothesis testing is used in machine learning for tasks such as evaluating the performance of a model, comparing different models, and detecting outliers.
- Clustering: This area of statistics deals with grouping data points into clusters based on similarities or dissimilarities. Clustering is used in machine learning for tasks such as unsupervised learning, dimensionality reduction, and feature extraction.
- Variational Inference: A method for approximating complex probability distributions using simpler distributions. In machine learning, variational inference is used to optimize models that have intractable posterior distributions, such as Bayesian neural networks.

## Information theory

Information theory is a branch of mathematics that deals with the quantification of information. In machine learning, information theory is used to evaluate the performance of models, such as cross-entropy loss, and to design algorithms, such as data compression and feature selection.

### Topics

- Entropy: A measure of the amount of uncertainty or randomness in a system. In machine learning, entropy can be used to evaluate the complexity of a model, the amount of information it captures, and the difficulty of learning from a given dataset.
- Mutual Information: A measure of the dependence between two random variables. In machine learning, mutual information is used to identify the most informative features, reduce dimensionality, and perform feature selection.
- Kullback-Leibler (KL) Divergence: A measure of the difference between two probability distributions. In machine learning, KL divergence is used to compare models, evaluate their performance, and select the best model for a given task.
