Vectors and Vector Spaces: Fundamental for understanding data structures in ML.
Matrices and Matrix Operations: Core to algorithms, especially in deep learning.
Eigenvalues and Eigenvectors: Important in PCA, SVD, and many optimization problems.
Linear Transformations: Fundamental for understanding data transformations.
Orthogonality and Least Squares: Key in optimization problems and for understanding distances and approximations.

Decomposition:
LU Decomposition: For solving linear equations and inverting matrices.
QR Decomposition: Used in solving linear least squares problems.
Cholesky Decomposition: Primarily used in optimization for symmetric, positive-definite matrices.
Eigenvalue Decomposition:  For eigenvalues and eigenvectors, used in PCA and similar techniques.
Singular Value Decomposition (SVD): Used in dimensionality reduction and data compression.

Matrix Factorization: Useful in recommendation systems and natural language processing.